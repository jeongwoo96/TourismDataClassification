{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, TFAutoModel, AdamWeightDecay, AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "class SAMModel(tf.keras.Model):\n",
    "    def __init__(self, my_model, rho=0.05):\n",
    "        \"\"\"\n",
    "        p, q = 2 for optimal results as suggested in the paper\n",
    "        (Section 2)\n",
    "        \"\"\"\n",
    "        super(SAMModel, self).__init__()\n",
    "        self.my_model = my_model\n",
    "        self.rho = rho\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (text, labels) = data\n",
    "        e_ws = []\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.my_model(text)\n",
    "            loss = self.compiled_loss(labels, predictions)\n",
    "        trainable_params = self.my_model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_params)\n",
    "        grad_norm = self._grad_norm(gradients)\n",
    "        scale = self.rho / (grad_norm + 1e-10)\n",
    "\n",
    "        for (grad, param) in zip(gradients, trainable_params):\n",
    "            e_w = grad * scale\n",
    "            param.assign_add(e_w)\n",
    "            e_ws.append(e_w)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.my_model(text)\n",
    "            loss = self.compiled_loss(labels, predictions)    \n",
    "        \n",
    "        sam_gradients = tape.gradient(loss, trainable_params)\n",
    "        for (param, e_w) in zip(trainable_params, e_ws):\n",
    "            param.assign_sub(e_w)\n",
    "        \n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(sam_gradients, trainable_params))\n",
    "        \n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (text, labels) = data\n",
    "        predictions = self.my_model(text, training=False)\n",
    "        loss = self.compiled_loss(labels, predictions)\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def _grad_norm(self, gradients):\n",
    "        norm = tf.norm(\n",
    "            tf.stack([\n",
    "                tf.norm(grad) for grad in gradients if grad is not None\n",
    "            ])\n",
    "        )\n",
    "        return norm  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of SAM.\n",
    "        SAM delegates the forward pass call to the wrapped model.\n",
    "        Args:\n",
    "          inputs: Tensor. The model inputs.\n",
    "        Returns:\n",
    "          A Tensor, the outputs of the wrapped model for given `inputs`.\n",
    "        \"\"\"\n",
    "        return self.my_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 모델 전처리 함수\n",
    "def preprocessing(train,val,test, modelname, max_seq_len, add_token=False, emphasize_token=False, token_change = False):\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = train['cat3']\n",
    "    y_val =  val['cat3']\n",
    "\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_val = encoder.transform(y_val)\n",
    "\n",
    "    y_train_data = pd.Series(y_train)\n",
    "    y_val_data = pd.Series(y_val)  \n",
    "    \n",
    "    model_name = modelname       \n",
    "    f = open('add_token.txt')        \n",
    "    add_token_ls = f.read().split()\n",
    "    emphasize_token_ls =  ['상설시장','채식주의','채식주의자','비건','비거니즘','고택','펜션','관아','팔각','주심포','건물','오일시장'] #반복을 통해 강조시켜줄 토큰\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if add_token == True :\n",
    "        for token in add_token_ls :\n",
    "            tokenizer.add_tokens(token)\n",
    "\n",
    "    def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "        \n",
    "        input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "        \n",
    "\n",
    "        for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "            if token_change == True :     # 특정 라벨에 특정 단어가 들어가는 경우 그 단어를 살작 변형 -> 사용했을때와 안했을때 모두 뽑아내서 앙상블\n",
    "                if label == '뮤지컬' :\n",
    "                    if example.find('뮤지컬') != -1 :\n",
    "                        example = example.replace('뮤지컬','뮤지컬(뮤지컬공연)')\n",
    "                if label == '분수' :\n",
    "                    if example.find('분수쇼') != -1 :\n",
    "                        example = example.replace('분수쇼','분수(분수쇼)')        \n",
    "                if label == '채식전문점' :\n",
    "                    if example.find('채식') != -1 :\n",
    "                        example = example.replace('채식','채식(채식전문점)')\n",
    "                if label == '게스트하우스' :\n",
    "                    if example.find('게스트하우스') != -1 :\n",
    "                        example = example.replace('게스트하우스','게스트하우스(게하)')        \n",
    "            \n",
    "\n",
    "            if emphasize_token == True :     #  특정 단어를 강조하기 위해 반복\n",
    "                for em in emphasize_token_ls :\n",
    "                    if example.find(em) != -1 :\n",
    "                        example = example.replace(em,'{} {} {}'.format(em,em,em))\n",
    "\n",
    "\n",
    "            input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)      # 토크나이저를 통해 인코딩\n",
    "            padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "            attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "            token_type_id = [0] * max_seq_len\n",
    "\n",
    "            assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "            assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "            assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "            token_type_ids.append(token_type_id)\n",
    "            data_labels.append(label)\n",
    "\n",
    "        input_ids = np.array(input_ids, dtype=int)\n",
    "        attention_masks = np.array(attention_masks, dtype=int)\n",
    "        token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "        data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "        return (input_ids, attention_masks, token_type_ids), data_labels  \n",
    "\n",
    "    y_test_data = [i for i in range(len(test))] #test data의 Y값 임의로 설정\n",
    "    x_train, y_train = convert_examples_to_features(train['overview'], y_train_data, max_seq_len=max_seq_len, tokenizer=tokenizer) \n",
    "    x_val, y_val = convert_examples_to_features(val['overview'], y_val_data, max_seq_len=max_seq_len, tokenizer=tokenizer) \n",
    "    x_test, _ = convert_examples_to_features(test['overview'], y_test_data, max_seq_len=max_seq_len, tokenizer=tokenizer) \n",
    "    return x_train, y_train, x_val, y_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inference(x_train, y_train, x_val, y_val, x_test, modelname, optimizer='Adam', dropout=0.1, TruncatedNormal=0.02, epocs=4, batch_size=16, sam = False):\n",
    "    \n",
    "    class TFBertForSequenceClassification(tf.keras.Model):\n",
    "        def __init__(self, model_name):\n",
    "            super(TFBertForSequenceClassification, self).__init__()\n",
    "            self.bert = TFAutoModel.from_pretrained(modelname, from_pt=True)\n",
    "            self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "            self.classifier = tf.keras.layers.Dense(128,\n",
    "                                                    kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                    activation='softmax',\n",
    "                                                    name='classifier')\n",
    "\n",
    "        def call(self, inputs):\n",
    "            input_ids, attention_mask, token_type_ids = inputs\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            cls_token = outputs[1]\n",
    "            prediction = self.classifier(cls_token)\n",
    "\n",
    "            return prediction    \n",
    "\n",
    "\n",
    "    model = TFBertForSequenceClassification(modelname)\n",
    "\n",
    "    if sam == True :\n",
    "        model = SAMModel(model)\n",
    "        \n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=epocs, batch_size=batch_size, validation_data=(x_val,y_val))\n",
    "\n",
    "    pred = model.predict(x_test, batch_size=batch_size) \n",
    "\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    return pred     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('final_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Model\n",
    "\n",
    "lr = 5e-6\n",
    "wd = 1e-2 * lr\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-6)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "val= pd.read_csv('val_set.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# 원본데이터, 사용자사전&강조&반복X\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
    "pred1 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=7,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=True,name = 'pred1')\n",
    "\n",
    "with open('final_result/pred1.pickle', 'wb') as f:\n",
    "    pickle.dump(pred1, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "# 원본데이터, 사용자사전&강조&반복O\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
    "pred2 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=8,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred2')\n",
    "with open('final_result/pred2.pickle', 'wb') as f:\n",
    "    pickle.dump(pred2, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train(adj,50).csv')   #형용사만 사용 증강\n",
    "\n",
    "\n",
    "# 형용사 이용 증강, 사용자사전&강조&반복X\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
    "pred3 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=8,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=False,name = 'pred3')\n",
    "with open('final_result/pred3.pickle', 'wb') as f:\n",
    "    pickle.dump(pred3, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# 형용사 이용 증강, 사용자사전&강조&반복O\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
    "pred4 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=8,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred4')\n",
    "with open('final_result/pred4.pickle', 'wb') as f:\n",
    "    pickle.dump(pred4, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train(ad,50).csv')   #부사만 사용 증강\n",
    "\n",
    "# 부사만 이용 증강, 사용자사전&강조&반복X\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
    "pred5 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=False,name = 'pred5')\n",
    "with open('final_result/pred5.pickle', 'wb') as f:\n",
    "    pickle.dump(pred5, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# 부사만 이용 증강, 사용자사전&강조&반복O\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
    "pred6 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred')\n",
    "with open('final_result/pred6.pickle', 'wb') as f:\n",
    "    pickle.dump(pred6, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train(adj,ad,50).csv')   #형용사&부사 사용 증강\n",
    "\n",
    "# 형용사&부사 이용 증강, 사용자사전&강조&반복X\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
    "pred7 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=True,name = 'pred7')\n",
    "with open('final_result/pred7.pickle', 'wb') as f:\n",
    "    pickle.dump(pred7, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# 형용사&부사 이용 증강, 사용자사전&강조&반복O\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=False,token_change=True)\n",
    "pred8 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred8')\n",
    "with open('final_result/pred8.pickle', 'wb') as f:\n",
    "    pickle.dump(pred8, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train(adj,ad,sim,100).csv')   #형용사&부사&유의어 사용 증강\n",
    "\n",
    "# 형용사&부사 이용 증강, 사용자사전&강조&반복X\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200)\n",
    "pred9 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.01,optimizer=optimizer,batch_size=8,sam=True,name = 'pred9')\n",
    "with open('final_result/pred9.pickle', 'wb') as f:\n",
    "    pickle.dump(pred9, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# 형용사&부사 이용 증강, 사용자사전&강조&반복O\n",
    "x_train, y_train, x_val, y_val, x_test = preprocessing(train,val,test,'klue/roberta-large',200,add_token=True,emphasize_token=True,token_change=True)\n",
    "pred10 = train_inference(x_train, y_train, x_val, y_val, x_test,'klue/roberta-large',epocs=6,dropout=0.1,TruncatedNormal=0.02,optimizer=optimizer,batch_size=8, sam=True,name = 'pred10')\n",
    "with open('final_result/pred10.pickle', 'wb') as f:\n",
    "    pickle.dump(pred10, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = train['cat3']\n",
    "\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n",
    "def mode(list):\n",
    "    count = 0\n",
    "    mode = 0\n",
    "    for x in list: \n",
    "        if list.count(x) > count:\n",
    "            count = list.count(x)\n",
    "            mode = x\n",
    "\n",
    "    return mode\n",
    "\n",
    "\n",
    "\n",
    "pred_ls = []\n",
    "for i in range(len(pred1)):\n",
    "  index = mode([\n",
    "      pred2[i].argmax(),\n",
    "      pred1[i].argmax(),\n",
    "      pred3[i].argmax(),\n",
    "      pred4[i].argmax(),\n",
    "      pred5[i].argmax(),\n",
    "      pred6[i].argmax(),\n",
    "      pred7[i].argmax(),\n",
    "      pred8[i].argmax(),\n",
    "      pred9[i].argmax(),\n",
    "      pred10[i].argmax(),\n",
    "      ])\n",
    "  pred_ls.append(encoder.classes_[index])\n",
    "y_pre= encoder.transform(pred_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['cat3'] = y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test[['id','cat3']]\n",
    "submission.to_csv('submission_text.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5102cbe6eef775fa675588fc485acc2fa38583d413d40ff78d2cfd2f494b2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
